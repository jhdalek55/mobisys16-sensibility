\section{Motivation}\label{sec-motivation}

%\textbf{Privacy and security risks.}
Due to recent privacy breaches and security break-ins to mobile systems, 
device security and personal privacy are genuinely at risk when a person 
uses a smartphone or tablet \cite{breach}. 
%Apps can post tweets to a 
%user's Twitter account without asking for permission~\cite{tweet}. 
A calculator might send the user's location to an advertisement 
server~\cite{calc}. Sensor data from the accelerometer 
or gyroscope can be sufficient to infer the locations of touch-screen 
taps, and thus infer a user's password~\cite{cai2011touchlogger}.
Compromised apps can even let criminals break into an individual's 
bank account~\cite{starbucks}. As a result, device owners 
are aware that running apps on their smartphones can raise privacy 
and security risks. 

However, having access to data from the enormous number of smartphones 
in use today could be tremendously valuable to the research 
community. As these devices belong to ordinary people, conducting
research on these devices does not incur high maintenance costs. 
Accelerometers on end-user devices could detect vibrations within 
the frequency and intensity range of seismic waves, and assist 
distributed earthquake detection~\cite{faulkner2011next}. GPS, 
WiFi, and cellular triangulation can be employed in distributed 
networks of sensors for traffic monitoring and accident 
prevention~\cite{mohan2008nericell, thiagarajan2009vtrack}. 
For the research community, accessing this data depends on its 
ability to provide strong protection to device owners from privacy 
and security breaches.

%\textbf{Restricting accessible data.}
%Despite these risks of using sensors, 
However, studies have indicated that 
sensor data can be accessed without compromising device 
owners' privacy or sacrificing too much service functioning.
A recent research study shows that more than half of the 
surveyed individuals had no problem in supplying imprecise 
sensor data from their personal devices~\cite{fawaz2014location}. 
Most participants could accommodate some inevitable loss of application 
functionality, as long as their privacy was protected. Those surveyed
applications ranged from location-based search (e.g., Yelp), social 
network apps, to gaming and weather forcasting apps. 
Researchers thus have proposed 
substituting mocked~\cite{beresford2011mockdroid}, anonymized 
or bogus~\cite{zhou2011taming} data in place of real data. Although
the accuracy of the data is reduced, experience has shown 
that the imprecise information is sufficient for a large class of 
services. The US Federal Communications Commission requires 
emergency rescue and 
response teams to be able to estimate a 911 wireless emergency 
caller's position with an accuracy of 125~m~\cite{gruteser2003anonymous, 
reed1998overview}. In other location-bases services such as maps, 
restaurant guides, and bus schedules, end users can still use the 
service even if a device only provides a discretized 
location~\cite{amini2011cache, krumm2007inference}. 
\yanyan{too many examples?}

Based on these fact, restricting 
the amount of data accessible, such as reducing the precision or 
access frequency, can be a good privacy protection mechanism we can 
provide to end users. In this work, we coin the term \textit{data blurring}
as our privacy protection mechanism. This mechanism is used in 
conjunction with a researcher's IRB policies. 
%where each data access
%policy is codified as a blurring layer, and different policies are
%customized by loading individual blurring layers in order.

%\textbf{Institutional review board (IRB).}
%On the other hand, research institutions have also designed a 
%protocol based on the \textit{institutional review board (IRB)}, 
%to assess the ethics of a researcher's project, and review its methods. 
Another issue we sought to address with Sensibility Testbed is to 
relieve the individual researcher of the need to manually enforce 
the restrictions set by his or her institution's IRB.
IRB, also known as an independent ethics committee (IEC), ethical 
review board (ERB), or research ethics board (REB), is a committee 
that has been formally designated to approve, monitor, and review 
research involving human subjects~\cite{irb}. Although many current network 
testbeds require that researchers obtain IRB approval before conducting
an experiment on the testbed, these platforms do not provide a guarantee 
for IRB policy compliance. In the case of PhoneLab, 
%it requires experimenters to obtain IRB approval. However, 
it leaves it up to the experimenters to comply with their IRB policies in their 
experiments~\cite{nandugudi2013phonelab}. Similarly, 
Mobilyzer~\cite{nikravesh2015mobilyzer} provides a measurement
library that can be included in Android apps. 
%and requires explicit user consent. 
There is no guarantee that an 
experiment will be compliant with the researcher's IRB policies.
%promotes fully informed 
%consent and voluntary participation by prospective subjects. 

Sensibility Testbed takes any researcher's IRB policies, and codifies 
them to restrict sensor access on an end-user's device to 
an institution's set access levels. 
%IRB plays a central role in defining the policies
%appropriate for research at individual institutions. Experimenters
%first obtain an IRB approval at their institution. Then with these IRB
%policies, Sensibility Testbed, as an intermediate, codifies the data access 
%regulations and enforces them at the end-user mobile devices. 
This is achieved through restricting data access  via
a set of blurring layers. Each layer implements an IRB policy by substituting 
approximate data in place of explicit, raw sensor data to the experiment code. Different layers 
together can be customized to cater to various institution's 
policies and regulations. Our goal is to facilitate the enforcement of 
IRB policies on behalf of researchers, such that experiments 
do not collect more data than needed to provide their functionalities.