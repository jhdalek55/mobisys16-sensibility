\section{Motivation and Background}\label{sec-motivation}

In this section, we present some background information on 
factors that shaped the design and deployment of 
Sensibility Testbed. First, we offer the motivations behind its 
development. 
%the desire to balance the benefits of such research 
%with the very real privacy risks that could potentially occur if a 
%device owner allows access to his/her device. 
Next, we look at one solution to the problem: collecting proximate 
data in place of real data. Third, we talk about 
the traditional role of IRB in research involving human subjects 
and to data accessed remotely. Lastly, we look at Sensibility Testbed's 
default policies, which automatically closes off access to 
sensors which have the highest risk for violating the privacy of 
device owners.

\textbf{Motivation: the potential and the risks of accessing sensor data.}
Having access to data from the enormous number of smartphones 
in use today could be tremendously valuable to researchers in a 
number of fields, ranging from health and fitness to the social sciences. 
%One study suggested that collecting sensor-based data is vastly 
%superior to the use of surveys, which can suffer from incomplete 
%content, biases, and a lack of continuity. Because survey-based 
%studies also are "fixed in time," there is no way to monitor behavior 
%over extended periods of time. 
It is also a research method that does not incur high maintenance costs. 
Unfortunately, it is a research method that is "expensive" in other ways, 
primarily time. Recruitment of volunteers is time-consuming and often 
frustrating. %There are a number of reasons why this recruitment is so 
%difficult. One is that 
First, there is no easy method to build a pool of willing device owners, 
so it needs to be re-done every time a new experiment is proposed. 
Second, there are complications due to institutional policies on work 
with human subjects (discussed later in this section) that effects who 
can be recruited and under what conditions. An individual recruiting 
on her own is likely to end up with a more homogenous set of subjects, 
drawn either from one locality or interest group. Last but not least, there 
is a very real threat of damage to a participant's device, and invasion 
of the participant's privacy. Participants face a significant risk if 
they offer their device to a researcher.

%Due to recent privacy breaches and security break-ins to mobile systems, 
%device security and personal privacy are genuinely at risk when a person 
%uses a smartphone or tablet \cite{breach}. 
%%Apps can post tweets to a 
%%user's Twitter account without asking for permission~\cite{tweet}. 
%A calculator app might send the user's location to an advertisement 
%server~\cite{calc}. Sensor data from the accelerometer 
%or gyroscope can be sufficient to infer the locations of touch-screen 
%taps, and thus infer a user's password~\cite{cai2011touchlogger}.
%Compromised apps can even let criminals break into an individual's 
%bank account~\cite{starbucks}. 
%%As a result, device owners 
%%are aware that running apps on their smartphones can raise privacy 
%%and security risks. 
%A major reason for the prevelant privacy breaches is that on many mobile 
%systems, such as Android, 
%only a sub-set of sensors like GPS and bluetooth are considered risky, 
%and their access is mediated~\cite{android-sec}. Other sensors 
%such as accelerometer, gyroscope, etc., 
%%are considered to be innocuous, 
%require no permission to access. Furthermore, %research shows that 
%device owners are often oblivious to the implications of granting access to
%a particular type of sensor or resource~\cite{felt2012android}. It is 
%therefore challenging to conduct research on end-user devices
%in compliance with ethical standards~\cite{zevenbergen2013ethical}.

%However, having access to data from the enormous number of smartphones 
%in use today could be tremendously valuable to the research 
%community. As these devices belong to ordinary people, conducting
%research on these devices does not incur high maintenance costs. 
%Accelerometers on end-user devices could detect vibrations within 
%the frequency and intensity range of seismic waves, and assist 
%distributed earthquake detection~\cite{faulkner2011next}. GPS, 
%WiFi, and cellular triangulation can be employed in distributed 
%networks of sensors for traffic monitoring and accident 
%prevention~\cite{mohan2008nericell, thiagarajan2009vtrack}. 
%For the research community, accessing this data depends on its 
%ability to provide strong protection to device owners from privacy 
%and security breaches. In this work, we try to address two issues. 

Solutions to the privacy and security problem have been difficult to 
identify, in part because the threat is not broadly recognized by device 
owners. Device owners may not fully 
understand the implications of granting access to a particular type of 
sensor or resource~\cite{felt2012android}. It is therefore challenging to 
conduct research on end-user devices in compliance with ethical standards, 
without an organized approach to enforcing those 
standards~\cite{zevenbergen2013ethical}. Another reason for 
the prevalent privacy breaches on many mobile systems %such as Android and iOS, 
is that only a sub-set of sensors like GPS and bluetooth are considered risky, 	
and have their access mediated~\cite{android-sec}. Other sensors 
such as accelerometer, gyroscope, etc., require no permission to access, 
thus leaving them open to attack. A first step to expanding the use of sensors on mobile devices is to design solutions that address the security needs of device owners and the practical needs of potential researchers.

\textbf{Restricted data access as privacy protection.}
%Despite these risks of using sensors, 
How specific does data need to be in order to be useful? Studies have indicated that 
sensor data can be accessed without compromising device 
owners' privacy or sacrificing service functioning if that data is generalized. 
Such data, that substitutes approximate location does not 
directly violate the privacy of the device owner, but still provides 
valuable information to the researcher conducting the study.
For this reason, researchers have proposed 
substituting mocked~\cite{beresford2011mockdroid} or 
anonymized~\cite{zhou2011taming} data in place of real data. 
For example, in location-based services such as maps, 
restaurant guides, and bus schedules, end users can still use the 
service even if a device only provides a discretized 
location~\cite{amini2011cache, krumm2007inference}. The use of 
generalized data could also encourage more volunteer participation. 
Recent research shows that more than half of the 
surveyed individuals had no problem in supplying imprecise 
sensor data from their personal devices~\cite{fawaz2014location}. 
Most participants could accommodate some reduced application 
functionality, as long as their privacy was protected. Those 
applications included in the survey
ranged from location-based search (e.g., Yelp), social 
network apps, to gaming and weather forecasting apps. 
%the imprecise information is sufficient for a large class of services. \lois{these two sentences have to be clarified. How does one survey an application? And, why would participants be willing to a loss of functionality?}
%The US Federal Communications Commission requires 
%emergency rescue and 
%response teams to be able to estimate a 911 wireless emergency 
%caller's position with an accuracy of 125~m~\cite{gruteser2003anonymous, 
%reed1998overview}. \yanyan{too many examples?}

Based on these facts, restricting 
the amount of data accessible, such as reducing the precision or 
access frequency, offer an effective privacy protection mechanism to 
provide to end users. In this work, we coin the term \textit{data blurring}
as one privacy protection mechanism. However, to achieve data blurring easily and reliably to 
ensure experiments do not collect more data than needed to provide 
their functionalities requires a way to automatically substitute approximate 
data in place of explicit raw data.

\textbf{IRB policies: guiding ethical behavior.}
%On the other hand, research institutions have also designed a 
%protocol based on the \textit{institutional review board (IRB)}, 
%to assess the ethics of a researcher's project, and review its methods. 
As mentioned earlier, every researcher's work will be guided by an 
Institutional Review Board\footnote{\scriptsize Also known as an 
independent ethics committee (IEC), ethical review board (ERB), 
or research ethics board (REB).}, an internal group that serves as 
the ethical watch dogs for colleges, universities, government agencies, 
and other research institutions. It is the job of these boards, 
to approve, monitor, and review research involving human 
subjects~\cite{irb}. While a commitment to ethical treatment of humans who submit to experiments has always been part of the professional codes of most scientists, IRBs did not become ubiquitous in research facilities until the latter part of the 20th century.  Partly provoked by atrocities committed by the Nazis in the name of scientific experiments in the Second World War II, and partly inspired by directives from the medical community, including Declaration of Helsinki established In 1964 by the World Medical Association, research institutions formally acknowledged the need to protect human subjects in any research setting. Today, IRBs require all researchers working under 
their aegis to submit the protocols of their studies in advance, with an aim to 
protect not only the physical and mental well-being of subjects, 
but also to protect any information about these individuals generated 
over the course of the study. However, enforcement of these policies becomes significantly harder when dealing with remote subjects. Although many current network 
testbeds require that researchers obtain IRB approval before conducting
an experiment on the testbed, these platforms do not provide a guarantee 
for IRB policy compliance~\cite{nandugudi2013phonelab, nikravesh2015mobilyzer}.
%In the case of PhoneLab, 
%it requires experimenters to obtain IRB approval. However, 
%it leaves it up to the experimenters to comply with their IRB policies in their 
%experiments~\cite{nandugudi2013phonelab, nikravesh2015mobilyzer}. 
%Similarly, Mobilyzer~\cite{nikravesh2015mobilyzer} provides a 
%measurement library that can be included in Android apps. 
%and requires explicit user consent. 
Therefore, there is no guarantee that an 
experiment will be compliant with a researcher's IRB policies. 
%promotes fully informed 
%consent and voluntary participation by prospective subjects. 

%IRB plays a central role in defining the policies
%appropriate for research at individual institutions. Experimenters
%first obtain an IRB approval at their institution. Then with these IRB
%policies, Sensibility Testbed, as an intermediate, codifies the data access 
%regulations and enforces them at the end-user mobile devices. 
%This is achieved through restricting data access  via
%a set of blurring layers. Each layer implements an IRB policy by substituting 
%approximate data in place of explicit, raw sensor data to the experiment code. Different layers 


%Our goal is to facilitate the enforcement of 
%IRB policies on behalf of researchers, and that experiments 
%do not collect more data than needed to provide their functionalities.
%This will also relieve researchers from the tedious work of 
%recruiting participants and enforcing IRB policies.

%In the domain of IRB, Alice and Bob are the participating subject, and 
%a researcher who conducts a research study on the subject, respectively.
%
%
%\textbf{Sensibility Testbed's default policies.}

\begin{table}
\scriptsize
\centering

\bgroup
\def\arraystretch{1.15}% % for table padding
\begin{tabular}{|l|c|c|c|}
\hline
\multirow{2}{*}{\bf Sensor} & 
\multicolumn{3}{c|}{\bf Default policy} \\\cline{2-4}
& {\bf LR} & {\bf MR} & {\bf HR} \\\hline

Battery (plug-in type, level, technology, etc.) & \tickmark &  & \\ \hline
Bluetooth (local name, scan mode, etc.) & & \tickmark & \\ \hline

\multirow{2}{5.5cm}{Cellular network (cell ID, area code, country code, 
operator name, etc.)} & & \multirow{2}{*}{\tickmark} & \\ 
& & & \\ \hline

Location (latitude, longitude, altitude, speed, etc.) & & \tickmark & \\ \hline
Settings (screen brightness, ringer volume, etc.) & & \tickmark & \\ \hline

\multirow{2}{5.5cm}{Motion sensors (accelerometer, 
gyroscope, magnetometer, orientation , etc.)} & & \multirow{2}{*}{\tickmark} & \\ 
& & & \\ \hline

\multirow{2}{5.5cm}{WiFi network (information about the 
currently active access point, and WiFi scan result)} & & \multirow{2}{*}{\tickmark} & \\ 
& & & \\ \hline 

%Start/stop activities & & & \xmark \\ \hline 
%Running applications & & & \xmark \\ \hline 
Camera (take pictures, record videos) & & & \xmark \\ \hline 
Intent (scan barcode, search, etc.) & & & \xmark \\ \hline 
Address book & & & \xmark \\ \hline 
Microphone (voice record) & & & \xmark \\ \hline 
SMS (send/receive messages, delete messages) & & & \xmark \\ \hline 

\end{tabular}
\egroup

\caption{\small Sensibility Testbed's default policies for sensors. LR/MR/HR
stands for low/moderate/high risk, respectively. Access is only allowed to sensors that have low to 
moderate risks (marked by \tickmark). Sensors that are highly risky are 
disabled by default (marked by \xmark).}
\label{tab:default}
%\vspace{-10pt}
\end{table}

\textbf{Sensibility Testbed's default policies.} %\label{sec-irb-policies}
\yanyan{this may fit well with design principles.}
The last unique concept Sensibility Testbed leveraged to ensure 
protection of end-user security and privacy protection is that all sensors 
are not alike. As mentioned earlier, failure to 
recognize the vulnerability of certain sensors was a key reason for privacy 
breaches. In designing Sensibility Testbed, default policies were set as 
to what types of sensors could be accessed. Even if an IRB happened 
to approve such a policy, there are certain sensors that the testbed's
own IRB designates as off-limits due to the high risk associated with 
potential breaches. 
%and for which access can be pre-approved with the
%researcher's local IRB. 
Only those sensors listed on our project 
wiki page~\cite{sensor-api} are accessible to a researcher. 
A summary of these sensors is listed in Table~\ref{tab:default}, 
with each one categorized as low, moderate or high 
privacy risk. The list of sensors that Sensibility Testbed provides are all of moderate 
to low privacy risks (marked by \tickmark), and the testbed further provides policy enforcement
(Section~\ref{sec-policy}) to protect all the sensor data. Sensors 
such as cameras and microphones that are deemed sensitive are not 
exposed to experiment code by default (marked by \xmark). Such 
classification is motivated by the Android system, where 
permissions are categorized into different protection levels~\cite{level}:
\textit{normal} permissions are automatically granted to the apps, 
\textit{dangerous} permissions are given based upon the 
user's consent, and so on. In our case, 
%we divide sensors into different risk levels, as shown 
%in Table~\ref{tab:default}. 
%Sensors with low to moderate risk are 
%allowed and protected by IRB policies. Sensors of high risk are 
%disabled by default. 
we divide sensors into different risk levels by the consequences and 
difficulties of a potential attack. If a microphone is controlled by 
a malicious party, it can be used to intelligently choose data of a 
higher value (e.g., credit card number, password) to record~\cite{zhang2015leave}. On the other 
hand, in order to infer a credit card number or password typed on a 
smartphone using motion sensors, the attack requires the installation of 
a sophisticated algorithm on the device that constantly learns about  
the patterns of data generated by accelerometer or gyroscope. In contrast,
using battery information alone is not sufficient to create a fingerprint 
for each device. Different information and mutiple occurrences need to
be pieced together to extract this data~\cite{battery-priv}. Therefore, 
compared to motion sensors, a microphone is considered a higher risk, 
and a battery is a significantly lower risk.

Although high-risk sensors are disabled, if such access  is critical to the 
study, access can be requested using a different IRB procedure. 
In this case, the research project has to go through the Sensibility 
Testbed's IRB, in addition to the researcher's IRB. 
\yanyan{if we think this is ok, then we provide specially
designed interface and policy?} \lois{following up on Yanyan's comment--If the Testbed's IRB says this expanded access is permissable, are the device owner's notified and can they opt out of this study? Otherwise, that would be a direct violation of the privacy protection you claim to give them}
\lois{I did not touch these last two paragraphs because I still don't know about  the opt-out policy for individuals if this permission is given}
%Depending on the experiment description provided by the 
%researcher, the fields marked with a (*) are the ones that will be blurred.
%
%

As a result, Sensibility Testbed does not
provide unfettered access to all sensors. 
%Access to sensors of
%higher risk, e.g., the policies that request restricted sensor data, 
%or at higher frequencies than our default policies, 
%needs to go through the Sensibility Testbed's IRB,
%in addition to the researcher's IRB. 
The default policies serve as a common denominator to all 
researchers' IRB policies. In most cases, we expect
that researchers need only go through their local IRB to get
the sensor access they need for their experiment. 
